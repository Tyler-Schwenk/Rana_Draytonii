{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBJNZUNNM7dqqzvU6fhafV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tyler-Schwenk/Rana_Draytonii/blob/main/ASTtraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**clone the repo**"
      ],
      "metadata": {
        "id": "M3vtsdTCsXdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEq6WnwksQg_",
        "outputId": "e1c5cd65-5459-46f8-b5ba-3dae0da084a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ast'...\n",
            "remote: Enumerating objects: 649, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 649 (delta 85), reused 130 (delta 77), pack-reused 503\u001b[K\n",
            "Receiving objects: 100% (649/649), 2.42 MiB | 7.99 MiB/s, done.\n",
            "Resolving deltas: 100% (345/345), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/YuanGongND/ast\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSZ8-q_MspZZ",
        "outputId": "36af0df2-87f5-46a9-e3ea-36b9e2e5f209"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ast  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**example run.sh(from ausioset), now changed:**"
      ],
      "metadata": {
        "id": "zfFsL4h7WzwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "\n",
        "set -x\n",
        "source ../../venvast/bin/activate\n",
        "export TORCH_HOME=../../pretrained_models\n",
        "\n",
        "model=ast\n",
        "dataset=Rana_Draytonii # replace with your dataset name\n",
        "set=balanced # replace with your training set type (full/balanced)\n",
        "imagenetpretrain=True\n",
        "\n",
        "# Please adjust all these parameters based on your data and task\n",
        "lr=1e-5\n",
        "epoch=5\n",
        "tr_data=train_data.json # replace with your training data path\n",
        "te_data=val_data.json # replace with your evaluation data path\n",
        "freqm=48\n",
        "timem=200\n",
        "mixup=0.5\n",
        "fstride=10\n",
        "tstride=10\n",
        "batch_size=12\n",
        "dataset_mean=-4.27 # same as Audioset\n",
        "dataset_std=4.57 # same as Audioset\n",
        "audio_length=1000 # 10 seconds\n",
        "noise=False\n",
        "metrics=acc\n",
        "loss=BCE\n",
        "warmup=True\n",
        "wa=True\n",
        "exp_dir=./exp/test-${set}-f$fstride-t$tstride-p$imagenetpretrain-b$batch_size-lr${lr}-decoupe\n",
        "\n",
        "if [ -d $exp_dir ]; then\n",
        "  echo 'exp exist'\n",
        "  exit\n",
        "fi\n",
        "mkdir -p $exp_dir\n",
        "\n",
        "python -W ignore ../../src/run.py --model ${model} --dataset ${dataset} \\\n",
        "--data-train ${tr_data} --data-val ${te_data} --exp-dir $exp_dir \\\n",
        "--label-csv ./data/class_labels_indices.csv --n_class 2 # replace with the number of classes in your task\n",
        "--lr $lr --n-epochs ${epoch} --batch-size $batch_size --save_model True \\\n",
        "--freqm $freqm --timem $timem --mixup ${mixup} \\\n",
        "--tstride $tstride --fstride $fstride --imagenet_pretrain $imagenetpretrain \\\n",
        "--dataset_mean ${dataset_mean} --dataset_std ${dataset_std} --audio_length ${audio_length} --noise ${noise} \\\n",
        "--metrics ${metrics} --loss ${loss} --warmup ${warmup} \\\n",
        "--wa ${wa}"
      ],
      "metadata": {
        "id": "YIziymfRWpno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ast_models.py:**"
      ],
      "metadata": {
        "id": "j510klDrV2GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Time    : 6/10/21 5:04 PM\n",
        "# Author  : Yuan Gong\n",
        "# Affiliation  : Massachusetts Institute of Technology\n",
        "# Email   : yuangong@mit.edu\n",
        "# File    : ast_models.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast\n",
        "import os\n",
        "import wget\n",
        "os.environ['TORCH_HOME'] = '../../pretrained_models'\n",
        "import timm\n",
        "from timm.models.layers import to_2tuple,trunc_normal_\n",
        "\n",
        "# override the timm package to relax the input shape constraint.\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class ASTModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The AST model.\n",
        "    :param label_dim: the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n",
        "    :param fstride: the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n",
        "    :param tstride: the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n",
        "    :param input_fdim: the number of frequency bins of the input spectrogram\n",
        "    :param input_tdim: the number of time frames of the input spectrogram\n",
        "    :param imagenet_pretrain: if use ImageNet pretrained model\n",
        "    :param audioset_pretrain: if use full AudioSet and ImageNet pretrained model\n",
        "    :param model_size: the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n",
        "    \"\"\"\n",
        "    def __init__(self, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=True, audioset_pretrain=True, model_size='base384', verbose=True):\n",
        "\n",
        "        super(ASTModel, self).__init__()\n",
        "        assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n",
        "\n",
        "        if verbose == True:\n",
        "            print('---------------AST Model Summary---------------')\n",
        "            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n",
        "        # override timm input shape restriction\n",
        "        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n",
        "\n",
        "        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n",
        "        if audioset_pretrain == False:\n",
        "            if model_size == 'tiny224':\n",
        "                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
        "            elif model_size == 'small224':\n",
        "                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
        "            elif model_size == 'base224':\n",
        "                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
        "            elif model_size == 'base384':\n",
        "                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n",
        "            else:\n",
        "                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n",
        "            self.original_num_patches = self.v.patch_embed.num_patches\n",
        "            self.oringal_hw = int(self.original_num_patches ** 0.5)\n",
        "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
        "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
        "\n",
        "            # automatcially get the intermediate shape\n",
        "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
        "            num_patches = f_dim * t_dim\n",
        "            self.v.patch_embed.num_patches = num_patches\n",
        "            if verbose == True:\n",
        "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
        "                print('number of patches={:d}'.format(num_patches))\n",
        "\n",
        "            # the linear projection layer\n",
        "            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
        "            if imagenet_pretrain == True:\n",
        "                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n",
        "                new_proj.bias = self.v.patch_embed.proj.bias\n",
        "            self.v.patch_embed.proj = new_proj\n",
        "\n",
        "            # the positional embedding\n",
        "            if imagenet_pretrain == True:\n",
        "                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n",
        "                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n",
        "                # cut (from middle) or interpolate the second dimension of the positional embedding\n",
        "                if t_dim <= self.oringal_hw:\n",
        "                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw / 2) - int(t_dim / 2): int(self.oringal_hw / 2) - int(t_dim / 2) + t_dim]\n",
        "                else:\n",
        "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n",
        "                # cut (from middle) or interpolate the first dimension of the positional embedding\n",
        "                if f_dim <= self.oringal_hw:\n",
        "                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw / 2) - int(f_dim / 2): int(self.oringal_hw / 2) - int(f_dim / 2) + f_dim, :]\n",
        "                else:\n",
        "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
        "                # flatten the positional embedding\n",
        "                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n",
        "                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n",
        "                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
        "            else:\n",
        "                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n",
        "                # TODO can use sinusoidal positional embedding instead\n",
        "                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n",
        "                self.v.pos_embed = new_pos_embed\n",
        "                trunc_normal_(self.v.pos_embed, std=.02)\n",
        "\n",
        "        # now load a model that is pretrained on both ImageNet and AudioSet\n",
        "        elif audioset_pretrain == True:\n",
        "            if audioset_pretrain == True and imagenet_pretrain == False:\n",
        "                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n",
        "            if model_size != 'base384':\n",
        "                raise ValueError('currently only has base384 AudioSet pretrained model.')\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            if os.path.exists('../../pretrained_models/audioset_10_10_0.4593.pth') == False:\n",
        "                # this model performs 0.4593 mAP on the audioset eval set\n",
        "                audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n",
        "                wget.download(audioset_mdl_url, out='../../pretrained_models/audioset_10_10_0.4593.pth')\n",
        "            sd = torch.load('../../pretrained_models/audioset_10_10_0.4593.pth', map_location=device)\n",
        "            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n",
        "            audio_model = torch.nn.DataParallel(audio_model)\n",
        "            audio_model.load_state_dict(sd, strict=False)\n",
        "            self.v = audio_model.module.v\n",
        "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
        "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
        "\n",
        "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
        "            num_patches = f_dim * t_dim\n",
        "            self.v.patch_embed.num_patches = num_patches\n",
        "            if verbose == True:\n",
        "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
        "                print('number of patches={:d}'.format(num_patches))\n",
        "\n",
        "            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n",
        "            # if the input sequence length is larger than the original audioset (10s), then cut the positional embedding\n",
        "            if t_dim < 101:\n",
        "                new_pos_embed = new_pos_embed[:, :, :, 50 - int(t_dim/2): 50 - int(t_dim/2) + t_dim]\n",
        "            # otherwise interpolate\n",
        "            else:\n",
        "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(12, t_dim), mode='bilinear')\n",
        "            if f_dim < 12:\n",
        "                new_pos_embed = new_pos_embed[:, :, 6 - int(f_dim/2): 6 - int(f_dim/2) + f_dim, :]\n",
        "            # otherwise interpolate\n",
        "            elif f_dim > 12:\n",
        "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
        "            new_pos_embed = new_pos_embed.reshape(1, 768, num_patches).transpose(1, 2)\n",
        "            self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
        "\n",
        "    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n",
        "        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n",
        "        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
        "        test_out = test_proj(test_input)\n",
        "        f_dim = test_out.shape[2]\n",
        "        t_dim = test_out.shape[3]\n",
        "        return f_dim, t_dim\n",
        "\n",
        "    @autocast()\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
        "        :return: prediction\n",
        "        \"\"\"\n",
        "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = x.transpose(2, 3)\n",
        "\n",
        "        B = x.shape[0]\n",
        "        x = self.v.patch_embed(x)\n",
        "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
        "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
        "        x = x + self.v.pos_embed\n",
        "        x = self.v.pos_drop(x)\n",
        "        for blk in self.v.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.v.norm(x)\n",
        "        x = (x[:, 0] + x[:, 1]) / 2\n",
        "\n",
        "        x = self.mlp_head(x)\n",
        "        return x\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_tdim = 100\n",
        "    ast_mdl = ASTModel(input_tdim=input_tdim)\n",
        "    # input a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins\n",
        "    test_input = torch.rand([10, input_tdim, 128])\n",
        "    test_output = ast_mdl(test_input)\n",
        "    # output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes.\n",
        "    print(test_output.shape)\n",
        "\n",
        "    input_tdim = 256\n",
        "    ast_mdl = ASTModel(input_tdim=input_tdim,label_dim=50, audioset_pretrain=True)\n",
        "    # input a batch of 10 spectrogram, each with 512 time frames and 128 frequency bins\n",
        "    test_input = torch.rand([10, input_tdim, 128])\n",
        "    test_output = ast_mdl(test_input)\n",
        "    # output should be in shape [10, 50], i.e., 10 samples, each with prediction of 50 classes.\n",
        "    print(test_output.shape)"
      ],
      "metadata": {
        "id": "venZ10vyU0jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**traintest.py:**"
      ],
      "metadata": {
        "id": "pjr8iPUgUyrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Time    : 6/10/21 11:00 PM\n",
        "# Author  : Yuan Gong\n",
        "# Affiliation  : Massachusetts Institute of Technology\n",
        "# Email   : yuangong@mit.edu\n",
        "# File    : traintest.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import datetime\n",
        "sys.path.append(os.path.dirname(os.path.dirname(sys.path[0])))\n",
        "from utilities import *\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import pickle\n",
        "from torch.cuda.amp import autocast,GradScaler\n",
        "\n",
        "def train(audio_model, train_loader, test_loader, args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print('running on ' + str(device))\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    # Initialize all of the statistics we want to keep track of\n",
        "    batch_time = AverageMeter()\n",
        "    per_sample_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    per_sample_data_time = AverageMeter()\n",
        "    loss_meter = AverageMeter()\n",
        "    per_sample_dnn_time = AverageMeter()\n",
        "    progress = []\n",
        "    # best_cum_mAP is checkpoint ensemble from the first epoch to the best epoch\n",
        "    best_epoch, best_cum_epoch, best_mAP, best_acc, best_cum_mAP = 0, 0, -np.inf, -np.inf, -np.inf\n",
        "    global_step, epoch = 0, 0\n",
        "    start_time = time.time()\n",
        "    exp_dir = args.exp_dir\n",
        "\n",
        "    def _save_progress():\n",
        "        progress.append([epoch, global_step, best_epoch, best_mAP,\n",
        "                time.time() - start_time])\n",
        "        with open(\"%s/progress.pkl\" % exp_dir, \"wb\") as f:\n",
        "            pickle.dump(progress, f)\n",
        "\n",
        "    if not isinstance(audio_model, nn.DataParallel):\n",
        "        audio_model = nn.DataParallel(audio_model)\n",
        "\n",
        "    audio_model = audio_model.to(device)\n",
        "    # Set up the optimizer\n",
        "    trainables = [p for p in audio_model.parameters() if p.requires_grad]\n",
        "    print('Total parameter number is : {:.3f} million'.format(sum(p.numel() for p in audio_model.parameters()) / 1e6))\n",
        "    print('Total trainable parameter number is : {:.3f} million'.format(sum(p.numel() for p in trainables) / 1e6))\n",
        "    optimizer = torch.optim.Adam(trainables, args.lr, weight_decay=5e-7, betas=(0.95, 0.999))\n",
        "\n",
        "    # dataset specific settings\n",
        "    main_metrics = args.metrics\n",
        "    if args.loss == 'BCE':\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "    elif args.loss == 'CE':\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "    warmup = args.warmup\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(args.lrscheduler_start, 1000, args.lrscheduler_step)),gamma=args.lrscheduler_decay)\n",
        "    args.loss_fn = loss_fn\n",
        "    print('now training with {:s}, main metrics: {:s}, loss function: {:s}, learning rate scheduler: {:s}'.format(str(args.dataset), str(main_metrics), str(loss_fn), str(scheduler)))\n",
        "    print('The learning rate scheduler starts at {:d} epoch with decay rate of {:.3f} every {:d} epochs'.format(args.lrscheduler_start, args.lrscheduler_decay, args.lrscheduler_step))\n",
        "\n",
        "    # 11/30/22: I decouple the dataset and the following hyper-parameters to make it easier to adapt to new datasets\n",
        "    # if args.dataset == 'audioset':\n",
        "    #     if len(train_loader.dataset) > 2e5:\n",
        "    #         print('scheduler for full audioset is used')\n",
        "    #         scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2,3,4,5], gamma=0.5, last_epoch=-1)\n",
        "    #     else:\n",
        "    #         print('scheduler for balanced audioset is used')\n",
        "    #         scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [10, 15, 20, 25], gamma=0.5, last_epoch=-1)\n",
        "    #     main_metrics = 'mAP'\n",
        "    #     loss_fn = nn.BCEWithLogitsLoss()\n",
        "    #     warmup = True\n",
        "    # elif args.dataset == 'esc50':\n",
        "    #     print('scheduler for esc-50 is used')\n",
        "    #     scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(5,26)), gamma=0.85)\n",
        "    #     main_metrics = 'acc'\n",
        "    #     loss_fn = nn.CrossEntropyLoss()\n",
        "    #     warmup = False\n",
        "    # elif args.dataset == 'speechcommands':\n",
        "    #     print('scheduler for speech commands is used')\n",
        "    #     scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(5,26)), gamma=0.85)\n",
        "    #     main_metrics = 'acc'\n",
        "    #     loss_fn = nn.BCEWithLogitsLoss()\n",
        "    #     warmup = False\n",
        "    # else:\n",
        "    #     raise ValueError('unknown dataset, dataset should be in [audioset, speechcommands, esc50]')\n",
        "\n",
        "    epoch += 1\n",
        "    # for amp\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    print(\"current #steps=%s, #epochs=%s\" % (global_step, epoch))\n",
        "    print(\"start training...\")\n",
        "    result = np.zeros([args.n_epochs, 10])\n",
        "    audio_model.train()\n",
        "    while epoch < args.n_epochs + 1:\n",
        "        begin_time = time.time()\n",
        "        end_time = time.time()\n",
        "        audio_model.train()\n",
        "        print('---------------')\n",
        "        print(datetime.datetime.now())\n",
        "        print(\"current #epochs=%s, #steps=%s\" % (epoch, global_step))\n",
        "\n",
        "        for i, (audio_input, labels) in enumerate(train_loader):\n",
        "\n",
        "            B = audio_input.size(0)\n",
        "            audio_input = audio_input.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            data_time.update(time.time() - end_time)\n",
        "            per_sample_data_time.update((time.time() - end_time) / audio_input.shape[0])\n",
        "            dnn_start_time = time.time()\n",
        "\n",
        "            # first several steps for warm-up\n",
        "            if global_step <= 1000 and global_step % 50 == 0 and warmup == True:\n",
        "                warm_lr = (global_step / 1000) * args.lr\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = warm_lr\n",
        "                print('warm-up learning rate is {:f}'.format(optimizer.param_groups[0]['lr']))\n",
        "\n",
        "            with autocast():\n",
        "                audio_output = audio_model(audio_input)\n",
        "                if isinstance(loss_fn, torch.nn.CrossEntropyLoss):\n",
        "                    loss = loss_fn(audio_output, torch.argmax(labels.long(), axis=1))\n",
        "                else:\n",
        "                    loss = loss_fn(audio_output, labels)\n",
        "\n",
        "            # optimization if amp is not used\n",
        "            # optimizer.zero_grad()\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            # optimiztion if amp is used\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # record loss\n",
        "            loss_meter.update(loss.item(), B)\n",
        "            batch_time.update(time.time() - end_time)\n",
        "            per_sample_time.update((time.time() - end_time)/audio_input.shape[0])\n",
        "            per_sample_dnn_time.update((time.time() - dnn_start_time)/audio_input.shape[0])\n",
        "\n",
        "            print_step = global_step % args.n_print_steps == 0\n",
        "            early_print_step = epoch == 0 and global_step % (args.n_print_steps/10) == 0\n",
        "            print_step = print_step or early_print_step\n",
        "\n",
        "            if print_step and global_step != 0:\n",
        "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Per Sample Total Time {per_sample_time.avg:.5f}\\t'\n",
        "                  'Per Sample Data Time {per_sample_data_time.avg:.5f}\\t'\n",
        "                  'Per Sample DNN Time {per_sample_dnn_time.avg:.5f}\\t'\n",
        "                  'Train Loss {loss_meter.avg:.4f}\\t'.format(\n",
        "                   epoch, i, len(train_loader), per_sample_time=per_sample_time, per_sample_data_time=per_sample_data_time,\n",
        "                      per_sample_dnn_time=per_sample_dnn_time, loss_meter=loss_meter), flush=True)\n",
        "                if np.isnan(loss_meter.avg):\n",
        "                    print(\"training diverged...\")\n",
        "                    return\n",
        "\n",
        "            end_time = time.time()\n",
        "            global_step += 1\n",
        "\n",
        "        print('start validation')\n",
        "        stats, valid_loss = validate(audio_model, test_loader, args, epoch)\n",
        "\n",
        "        # ensemble results\n",
        "        cum_stats = validate_ensemble(args, epoch)\n",
        "        cum_mAP = np.mean([stat['AP'] for stat in cum_stats])\n",
        "        cum_mAUC = np.mean([stat['auc'] for stat in cum_stats])\n",
        "        cum_acc = cum_stats[0]['acc']\n",
        "\n",
        "        mAP = np.mean([stat['AP'] for stat in stats])\n",
        "        mAUC = np.mean([stat['auc'] for stat in stats])\n",
        "        acc = stats[0]['acc']\n",
        "\n",
        "        middle_ps = [stat['precisions'][int(len(stat['precisions'])/2)] for stat in stats]\n",
        "        middle_rs = [stat['recalls'][int(len(stat['recalls'])/2)] for stat in stats]\n",
        "        average_precision = np.mean(middle_ps)\n",
        "        average_recall = np.mean(middle_rs)\n",
        "\n",
        "        if main_metrics == 'mAP':\n",
        "            print(\"mAP: {:.6f}\".format(mAP))\n",
        "        else:\n",
        "            print(\"acc: {:.6f}\".format(acc))\n",
        "        print(\"AUC: {:.6f}\".format(mAUC))\n",
        "        print(\"Avg Precision: {:.6f}\".format(average_precision))\n",
        "        print(\"Avg Recall: {:.6f}\".format(average_recall))\n",
        "        print(\"d_prime: {:.6f}\".format(d_prime(mAUC)))\n",
        "        print(\"train_loss: {:.6f}\".format(loss_meter.avg))\n",
        "        print(\"valid_loss: {:.6f}\".format(valid_loss))\n",
        "\n",
        "        if main_metrics == 'mAP':\n",
        "            result[epoch-1, :] = [mAP, mAUC, average_precision, average_recall, d_prime(mAUC), loss_meter.avg, valid_loss, cum_mAP, cum_mAUC, optimizer.param_groups[0]['lr']]\n",
        "        else:\n",
        "            result[epoch-1, :] = [acc, mAUC, average_precision, average_recall, d_prime(mAUC), loss_meter.avg, valid_loss, cum_acc, cum_mAUC, optimizer.param_groups[0]['lr']]\n",
        "        np.savetxt(exp_dir + '/result.csv', result, delimiter=',')\n",
        "        print('validation finished')\n",
        "\n",
        "        if mAP > best_mAP:\n",
        "            best_mAP = mAP\n",
        "            if main_metrics == 'mAP':\n",
        "                best_epoch = epoch\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            if main_metrics == 'acc':\n",
        "                best_epoch = epoch\n",
        "\n",
        "        if cum_mAP > best_cum_mAP:\n",
        "            best_cum_epoch = epoch\n",
        "            best_cum_mAP = cum_mAP\n",
        "\n",
        "        if best_epoch == epoch:\n",
        "            torch.save(audio_model.state_dict(), \"%s/models/best_audio_model.pth\" % (exp_dir))\n",
        "            torch.save(optimizer.state_dict(), \"%s/models/best_optim_state.pth\" % (exp_dir))\n",
        "\n",
        "        torch.save(audio_model.state_dict(), \"%s/models/audio_model.%d.pth\" % (exp_dir, epoch))\n",
        "        if len(train_loader.dataset) > 2e5:\n",
        "            torch.save(optimizer.state_dict(), \"%s/models/optim_state.%d.pth\" % (exp_dir, epoch))\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "        with open(exp_dir + '/stats_' + str(epoch) +'.pickle', 'wb') as handle:\n",
        "            pickle.dump(stats, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        _save_progress()\n",
        "\n",
        "        finish_time = time.time()\n",
        "        print('epoch {:d} training time: {:.3f}'.format(epoch, finish_time-begin_time))\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "        batch_time.reset()\n",
        "        per_sample_time.reset()\n",
        "        data_time.reset()\n",
        "        per_sample_data_time.reset()\n",
        "        loss_meter.reset()\n",
        "        per_sample_dnn_time.reset()\n",
        "\n",
        "    # if args.dataset == 'audioset':\n",
        "    #     if len(train_loader.dataset) > 2e5:\n",
        "    #         stats=validate_wa(audio_model, test_loader, args, 1, 5)\n",
        "    #     else:\n",
        "    #         stats=validate_wa(audio_model, test_loader, args, 6, 25)\n",
        "    if args.wa == True:\n",
        "        stats = validate_wa(audio_model, test_loader, args, args.wa_start, args.wa_end)\n",
        "        mAP = np.mean([stat['AP'] for stat in stats])\n",
        "        mAUC = np.mean([stat['auc'] for stat in stats])\n",
        "        middle_ps = [stat['precisions'][int(len(stat['precisions'])/2)] for stat in stats]\n",
        "        middle_rs = [stat['recalls'][int(len(stat['recalls'])/2)] for stat in stats]\n",
        "        average_precision = np.mean(middle_ps)\n",
        "        average_recall = np.mean(middle_rs)\n",
        "        wa_result = [mAP, mAUC, average_precision, average_recall, d_prime(mAUC)]\n",
        "        print('---------------Training Finished---------------')\n",
        "        print('weighted averaged model results')\n",
        "        print(\"mAP: {:.6f}\".format(mAP))\n",
        "        print(\"AUC: {:.6f}\".format(mAUC))\n",
        "        print(\"Avg Precision: {:.6f}\".format(average_precision))\n",
        "        print(\"Avg Recall: {:.6f}\".format(average_recall))\n",
        "        print(\"d_prime: {:.6f}\".format(d_prime(mAUC)))\n",
        "        print(\"train_loss: {:.6f}\".format(loss_meter.avg))\n",
        "        print(\"valid_loss: {:.6f}\".format(valid_loss))\n",
        "        np.savetxt(exp_dir + '/wa_result.csv', wa_result)\n",
        "\n",
        "def validate(audio_model, val_loader, args, epoch):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    batch_time = AverageMeter()\n",
        "    if not isinstance(audio_model, nn.DataParallel):\n",
        "        audio_model = nn.DataParallel(audio_model)\n",
        "    audio_model = audio_model.to(device)\n",
        "    # switch to evaluate mode\n",
        "    audio_model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    A_predictions = []\n",
        "    A_targets = []\n",
        "    A_loss = []\n",
        "    with torch.no_grad():\n",
        "        for i, (audio_input, labels) in enumerate(val_loader):\n",
        "            audio_input = audio_input.to(device)\n",
        "\n",
        "            # compute output\n",
        "            audio_output = audio_model(audio_input)\n",
        "            audio_output = torch.sigmoid(audio_output)\n",
        "            predictions = audio_output.to('cpu').detach()\n",
        "\n",
        "            A_predictions.append(predictions)\n",
        "            A_targets.append(labels)\n",
        "\n",
        "            # compute the loss\n",
        "            labels = labels.to(device)\n",
        "            if isinstance(args.loss_fn, torch.nn.CrossEntropyLoss):\n",
        "                loss = args.loss_fn(audio_output, torch.argmax(labels.long(), axis=1))\n",
        "            else:\n",
        "                loss = args.loss_fn(audio_output, labels)\n",
        "            A_loss.append(loss.to('cpu').detach())\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "        audio_output = torch.cat(A_predictions)\n",
        "        target = torch.cat(A_targets)\n",
        "        loss = np.mean(A_loss)\n",
        "        stats = calculate_stats(audio_output, target)\n",
        "\n",
        "        # save the prediction here\n",
        "        exp_dir = args.exp_dir\n",
        "        if os.path.exists(exp_dir+'/predictions') == False:\n",
        "            os.mkdir(exp_dir+'/predictions')\n",
        "            np.savetxt(exp_dir+'/predictions/target.csv', target, delimiter=',')\n",
        "        np.savetxt(exp_dir+'/predictions/predictions_' + str(epoch) + '.csv', audio_output, delimiter=',')\n",
        "\n",
        "    return stats, loss\n",
        "\n",
        "def validate_ensemble(args, epoch):\n",
        "    exp_dir = args.exp_dir\n",
        "    target = np.loadtxt(exp_dir+'/predictions/target.csv', delimiter=',')\n",
        "    if epoch == 1:\n",
        "        cum_predictions = np.loadtxt(exp_dir + '/predictions/predictions_1.csv', delimiter=',')\n",
        "    else:\n",
        "        cum_predictions = np.loadtxt(exp_dir + '/predictions/cum_predictions.csv', delimiter=',') * (epoch - 1)\n",
        "        predictions = np.loadtxt(exp_dir+'/predictions/predictions_' + str(epoch) + '.csv', delimiter=',')\n",
        "        cum_predictions = cum_predictions + predictions\n",
        "        # remove the prediction file to save storage space\n",
        "        os.remove(exp_dir+'/predictions/predictions_' + str(epoch-1) + '.csv')\n",
        "\n",
        "    cum_predictions = cum_predictions / epoch\n",
        "    np.savetxt(exp_dir+'/predictions/cum_predictions.csv', cum_predictions, delimiter=',')\n",
        "\n",
        "    stats = calculate_stats(cum_predictions, target)\n",
        "    return stats\n",
        "\n",
        "def validate_wa(audio_model, val_loader, args, start_epoch, end_epoch):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    exp_dir = args.exp_dir\n",
        "\n",
        "    sdA = torch.load(exp_dir + '/models/audio_model.' + str(start_epoch) + '.pth', map_location=device)\n",
        "\n",
        "    model_cnt = 1\n",
        "    for epoch in range(start_epoch+1, end_epoch+1):\n",
        "        sdB = torch.load(exp_dir + '/models/audio_model.' + str(epoch) + '.pth', map_location=device)\n",
        "        for key in sdA:\n",
        "            sdA[key] = sdA[key] + sdB[key]\n",
        "        model_cnt += 1\n",
        "\n",
        "        # if choose not to save models of epoch, remove to save space\n",
        "        if args.save_model == False:\n",
        "            os.remove(exp_dir + '/models/audio_model.' + str(epoch) + '.pth')\n",
        "\n",
        "    # averaging\n",
        "    for key in sdA:\n",
        "        sdA[key] = sdA[key] / float(model_cnt)\n",
        "\n",
        "    audio_model.load_state_dict(sdA)\n",
        "\n",
        "    torch.save(audio_model.state_dict(), exp_dir + '/models/audio_model_wa.pth')\n",
        "\n",
        "    stats, loss = validate(audio_model, val_loader, args, 'wa')\n",
        "    return stats"
      ],
      "metadata": {
        "id": "1Un1UgrgsuK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}